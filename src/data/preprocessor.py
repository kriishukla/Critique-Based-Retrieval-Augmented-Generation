"""
Data preprocessing module for loading and preparing datasets.
"""
from typing import Dict, List, Any, Tuple
from pathlib import Path
import json

from ..utils.logger import get_logger

logger = get_logger(__name__)


def load_natural_questions(
    data_path: str,
    num_samples: int = 500
) -> Dict[str, List[Any]]:
    """
    Load NaturalQuestions dataset.
    
    Args:
        data_path: Path to NaturalQuestions data
        num_samples: Number of samples to load
        
    Returns:
        Dictionary with 'queries', 'answers', 'contexts', 'documents'
    """
    logger.info(f"Loading NaturalQuestions from {data_path}")
    
    
    raise NotImplementedError(
        "Dataset loading not yet implemented. "
        "Waiting for dataset source confirmation."
    )


def load_hotpot_qa(
    data_path: str,
    num_samples: int = 500
) -> Dict[str, List[Any]]:
    """
    Load HotpotQA dataset.
    
    Args:
        data_path: Path to HotpotQA data
        num_samples: Number of samples to load
        
    Returns:
        Dictionary with 'queries', 'answers', 'contexts', 'documents'
    """
    logger.info(f"Loading HotpotQA from {data_path}")
    
    
    raise NotImplementedError(
        "Dataset loading not yet implemented. "
        "Waiting for dataset source confirmation."
    )


def validate_dataset(data: Dict[str, List[Any]]) -> bool:
    """
    Validate dataset structure and content.
    
    Args:
        data: Dataset dictionary
        
    Returns:
        True if valid, raises ValueError otherwise
    """
    required_keys = ['queries', 'answers', 'contexts', 'documents']
    
    for key in required_keys:
        if key not in data:
            raise ValueError(f"Missing required key: {key}")
    
    lengths = [len(data[key]) for key in ['queries', 'answers', 'contexts']]
    if len(set(lengths)) > 1:
        raise ValueError(f"Inconsistent lengths: {dict(zip(['queries', 'answers', 'contexts'], lengths))}")
    
    logger.info(f"Dataset validated: {lengths[0]} samples")
    return True


def extract_documents(contexts: List[str]) -> List[str]:
    """
    Extract unique documents from contexts.
    
    Args:
        contexts: List of context strings
        
    Returns:
        List of unique document chunks
    """
    documents = list(set(contexts))
    logger.info(f"Extracted {len(documents)} unique documents from {len(contexts)} contexts")
    return documents


def save_processed_data(
    data: Dict[str, List[Any]],
    output_path: str
) -> None:
    """
    Save processed data to JSON file.
    
    Args:
        data: Processed dataset
        output_path: Path to save JSON file
    """
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Processed data saved to {output_path}")


def load_processed_data(input_path: str) -> Dict[str, List[Any]]:
    """
    Load processed data from JSON file.
    
    Args:
        input_path: Path to JSON file
        
    Returns:
        Loaded dataset
    """
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    logger.info(f"Loaded processed data from {input_path}")
    return data
